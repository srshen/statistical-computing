---
title: "Report of cdpoLasso"
author: "Shirun Shen"
date: "10/12/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Package description

"cdpoLasso" is an R package that contains two methods to solve Lasso problem. The first one is coordinate descent algorithm, and the function is cdLasso. The other one is proximal operator algorithm, and the function is poLasso. Both of these two functions have 4 parameters, including the design matrix X, the response variable Y, the real parameters beta_real, and the tuning parameter $\lambda$.

This report will show you how this package works.

## Data initialization

We generate the data as following:

```{r data_generating}

n = 100              ##number of observations
p = 500              ##dimension of parameters
sigma_noise = 0.05
beta = rep(0,p)
beta[1:6] = c(5,10,3,80,90,10)   ##real beta(parameters)

####generating the design matrix
XData = matrix(rnorm(n*p,sd=10),nrow=n,ncol=p) ##design matrix

####nomalizing the data, which is very important
for(i in 1:500){
  X.sum <- sum(XData[,i]);
  for(j in 1:100){
    XData[j,i] <- XData[j,i] - X.sum/100
  }
  X.var <- sum((XData[,i]^2))
  for(j in 1:100){
    XData[j,i] <- 10*XData[j,i]/sqrt(X.var)
  }
}

####generating response variables
YData = XData %*% beta + rnorm(n, sd = sigma_noise)
```
Notice that we generate data satisfying the important conditions
$$\begin{align}\sum y_i =& 0  \\
\sum x_{ij} =& 0, ~~j=1,\dots,p  \\
\sum x_{ij}^2 =& 1, ~~j=1,\dots, p
\end{align}$$

And if we want, we can also generate $X$ and $Y$ first, then normalize them. It would not be very difficult.

## data analysis with cdpoLasso

Next, we use the package we made before to analyze the data. The first one is using coordinate descent algorithm. The code is shown below.
```{r cdLasso}
library(cdpoLasso)

####call cdLasso function
res_cd <- cdLasso(XData,YData,beta,0.33); 

####parameter estimation for beta by using coordinate descent algorithm
betahat1 <- res_cd$parameters;

####difference between the real beta and the estimating betahat1
diff_cd <- res_cd$l1_difference;
```

The second one is using proximal operator algorithm. The code is shown below.
```{r poLasso}
####call poLasso function
res_po <- poLasso(XData, YData, beta, 0.25);  

####parameter estimation for beta by using proximal operator algorithm
betahat2 <- res_po$parameters;

####difference between the real beta and the estimating betahat2
diff_po <- res_po$l1_difference;

```

We can easily find that the estimating parameters $\hat{\beta}_1$ and $\hat{\beta}_2$ are both closed to the real $\beta$.

```{r betahat1}
betahat1[1:10]  ####obmit the remained 490 zero elements
```
```{r betahat2}
betahat2[1:10]  ####obmit the remained 490 zero elements
```
And we can find the $l_1$ difference between $\hat{\beta}_1$ and $\beta$, also the $l_1$ difference between $\hat{\beta}_2$ and $\beta$. We draw a plot to show the difference.
```{r plot}
plot(x=1:1200,log10(diff_cd),type="p",cex = 0.1,ylim = c(0,5),
     xlab ="iteration number",ylab = "l1_difference")
points(x=1:1200,log10(diff_po),cex = 0.1,col="red")
```

The black dots are the difference between $\beta$ and $\hat{\beta}_1$, which are generated by cdLasso. And the red dots are the difference between $\beta$ and $\hat{\beta}_2$, which are generated by poLasso.

According to the plot above, we find that the convergence rate of coordinate descent algorithm are faster than that of proximal operator algorithm. And sometimes, the accuracy of estimation of proximal operator algorithm is better than that of coordinate descent algorithm, given enough iteration numbers.
